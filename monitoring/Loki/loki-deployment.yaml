# /monitoring/Loki/loki-deployment.yaml
# envsubst placeholders resolved by deploy_loki.sh before kubectl apply.

apiVersion: v1
kind: Namespace
metadata:
  name: ${LOKI_NAMESPACE}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: loki-config
  namespace: ${LOKI_NAMESPACE}
data:
  loki.yaml: |
    auth_enabled: false

    server:
      http_listen_port: 3100
      grpc_listen_port: 9096
      http_listen_address: 0.0.0.0
      grpc_listen_address: 0.0.0.0

    ingester:
      lifecycler:
        ring:
          kvstore:
            store: inmemory
          replication_factor: 1
        # Use the pod's actual IP so the ring can route correctly.
        # ${POD_IP} is injected via the Downward API env var below.
        address: 0.0.0.0
        heartbeat_timeout: 1m
      chunk_idle_period: 5m
      chunk_retain_period: 30s
      max_transfer_retries: 0
      wal:
        enabled: false

    schema_config:
      configs:
        - from: 2023-01-01
          store: boltdb-shipper
          object_store: filesystem
          schema: v11
          index:
            prefix: index_
            period: 24h

    storage_config:
      boltdb_shipper:
        active_index_directory: /loki/index
        cache_location: /loki/cache
        cache_ttl: 24h
        shared_store: filesystem
      filesystem:
        directory: /loki/chunks

    compactor:
      working_directory: /loki/compactor
      shared_store: filesystem
      # Retention is enforced by the compactor in Loki 2.x.
      retention_enabled: true
      retention_delete_delay: 2h
      retention_delete_worker_count: 150

    limits_config:
      retention_period: ${LOKI_RETENTION_PERIOD}
      ingestion_rate_mb: 10
      ingestion_burst_size_mb: 20
      max_query_series: 1000
      # Required for retention to work with the compactor.
      allow_structured_metadata: false

    ruler:
      storage:
        type: local
        local:
          directory: /loki/rules
      rule_path: /loki/rules-temp
      alertmanager_url: http://localhost:9093

    analytics:
      reporting_enabled: false
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: loki-pvc
  namespace: ${LOKI_NAMESPACE}
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: ${LOKI_STORAGE_SIZE}
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: loki
  namespace: ${LOKI_NAMESPACE}
  labels:
    app: loki
spec:
  replicas: 1
  selector:
    matchLabels:
      app: loki
  template:
    metadata:
      labels:
        app: loki
    spec:
      securityContext:
        fsGroup: 10001
        runAsUser: 10001
        runAsNonRoot: true
      containers:
      - name: loki
        image: grafana/loki:${LOKI_VERSION}
        args:
          - -config.file=/etc/loki/loki.yaml
        ports:
        - name: http-metrics
          containerPort: 3100
          protocol: TCP
        - name: grpc
          containerPort: 9096
          protocol: TCP
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        livenessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          # increased from 45s — Loki 2.9.x with boltdb-shipper needs
          # extra time to compact the WAL and open the index on first start.
          # 45s was too short on resource-constrained nodes (Minikube 1 CPU),
          # causing kubelet to kill the pod before it finished initialising.
          initialDelaySeconds: 60
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 5
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 5
        resources:
          requests:
            cpu: ${LOKI_CPU_REQUEST}
            memory: ${LOKI_MEMORY_REQUEST}
          limits:
            cpu: ${LOKI_CPU_LIMIT}
            memory: ${LOKI_MEMORY_LIMIT}
        volumeMounts:
        - name: config
          mountPath: /etc/loki
        - name: storage
          mountPath: /loki
      volumes:
      - name: config
        configMap:
          name: loki-config
      - name: storage
        persistentVolumeClaim:
          claimName: loki-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: loki
  namespace: ${LOKI_NAMESPACE}
  labels:
    app: loki
spec:
  type: ClusterIP
  ports:
  - port: 3100
    targetPort: 3100
    protocol: TCP
    name: http-metrics
  - port: 9096
    targetPort: 9096
    protocol: TCP
    name: grpc
  selector:
    app: loki
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: promtail
  namespace: ${LOKI_NAMESPACE}
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: promtail
rules:
- apiGroups: [""]
  resources:
  - nodes
  - nodes/proxy
  - services
  - endpoints
  - pods
  verbs: ["get", "watch", "list"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: promtail
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: promtail
subjects:
- kind: ServiceAccount
  name: promtail
  namespace: ${LOKI_NAMESPACE}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: promtail-config
  namespace: ${LOKI_NAMESPACE}
data:
  promtail.yaml: |
    server:
      http_listen_port: 9080
      grpc_listen_port: 0

    positions:
      filename: /tmp/positions.yaml

    clients:
      - url: http://loki.${LOKI_NAMESPACE}.svc.cluster.local:3100/loki/api/v1/push
        backoff_config:
          min_period: 500ms
          max_period: 30s
          max_retries: 20

    scrape_configs:
    - job_name: kubernetes-pods
      kubernetes_sd_configs:
      - role: pod
      pipeline_stages:
      - docker: {}
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_node_name]
        target_label: __host__
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - action: replace
        replacement: $1
        separator: /
        source_labels:
        - __meta_kubernetes_namespace
        - __meta_kubernetes_pod_name
        target_label: job
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_name
        target_label: pod
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_container_name
        target_label: container
      - replacement: /var/log/pods/*$1/*.log
        separator: /
        source_labels:
        - __meta_kubernetes_pod_uid
        - __meta_kubernetes_pod_container_name
        target_label: __path__
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: promtail
  namespace: ${LOKI_NAMESPACE}
  labels:
    app: promtail
spec:
  selector:
    matchLabels:
      app: promtail
  template:
    metadata:
      labels:
        app: promtail
    spec:
      serviceAccountName: promtail
      # Must run as root to read host log files (owned by root).
      securityContext:
        runAsUser: 0
        runAsGroup: 0
      # Promtail and Loki are applied in the same kubectl apply, so Promtail
      # pods start before Loki's HTTP server accepts connections. The Service
      # endpoint is only added after Loki passes its readinessProbe (30s delay),
      # so for ~30-60s the ClusterIP returns "connection refused" — exactly what
      # the Promtail logs showed. This initContainer blocks Promtail from starting
      # until Loki is actually responding to /ready, eliminating the error burst.
      initContainers:
      - name: wait-for-loki
        image: busybox:1.36
        # wget is available in busybox without installing curl or other tools.
        # Polls /ready every 5s until Loki returns the string "ready".
        command:
          - sh
          - -c
          - |
            echo "Waiting for Loki to be ready..."
            until wget -qO- http://loki.${LOKI_NAMESPACE}.svc.cluster.local:3100/ready 2>/dev/null | grep -q "ready"; do
              echo "Loki not ready yet, retrying in 5s..."
              sleep 5
            done
            echo "Loki is ready. Starting Promtail."
        securityContext:
          runAsUser: 0
      containers:
      - name: promtail
        image: grafana/promtail:${LOKI_VERSION}
        args:
          - -config.file=/etc/promtail/promtail.yaml
        ports:
        - containerPort: 9080
          name: http-metrics
        env:
        - name: HOSTNAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        resources:
          requests:
            cpu: 50m
            memory: 128Mi
          limits:
            cpu: 200m
            memory: 256Mi
        volumeMounts:
        - name: config
          mountPath: /etc/promtail
        - name: varlog
          mountPath: /var/log
          readOnly: true
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
      volumes:
      - name: config
        configMap:
          name: promtail-config
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/master
        operator: Exists
      - effect: NoSchedule
        key: node-role.kubernetes.io/control-plane
        operator: Exists